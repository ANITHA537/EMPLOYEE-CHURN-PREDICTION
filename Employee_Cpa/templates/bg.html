<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css"/>
    <link rel="stylesheet" href="static\styles.css" />
   
  
  <link rel="shortcut icon" href="C:/Users/ANITHA/Desktop/Project/Employee_Cpa/file.jpg" type="image/x-icon">
    <style>
    .centered {
    position: absolute;
    top: 50%;
    left: 25%;
    transform: translate(-50%, -50%); 
    }
    .top-left {
    position: absolute;
    top: 15%;
    }
    .tl {
    position: absolute;
    top: 25%;
    left: 10%;
    }
    ul, ul.points {
                list-style: circle;
                list-style-position: initial;
                list-style-image: initial;
                list-style-type: circle;
                line-height: 150%;
    }
    ::marker {
    unicode-bidi: isolate;
    font-variant-numeric: tabular-nums;
    text-transform: none;
    text-indent: 0px !important;
    text-align: start !important;
    text-align-last: start !important;
}
</style>
  
    <title>EMPLOYEE CHURN PREDICTION</title>
  </head>
  <body>
    <div class="menu-bar">
      <h1 class="logo"><span> EMPLOYEE CHURN</span> PREDICTION </h1>
      <ul>
        <li><a href="Nav">Home</a></li>
        <li><a href="attributes">Required Attributes</a></li>
        <li><a href="index">Upload File Here</a></li>
        <li><a href="about">About</a></li>
      </ul>
    </div>
    <div>
      <div class="container">
        <img src="static\images\background7.jpg" height="100%" width="100%"/>
        <div class="top-left">
            <h2>BAGGING CLASSIFIER in Machine Learning:</h2>
            <br/>
            <p>A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.<br>Each base classifier is trained in parallel with a training set which is generated by randomly drawing, with replacement, N examples(or data) from the original training dataset â€“ <em>where N is the size of the original training set</em>. Training set for each of the base classifiers is independent of each other. Many of the original data may be repeated in the resulting training set while others may be left out.</p><br/>
            <p>Bagging reduces overfitting (variance) by averaging or voting, however, this leads to an increase in bias, which is compensated by the reduction in variance though.</p><br/>
            <p><strong>How Bagging works on training dataset ?</strong><br>How bagging works on an imaginary training dataset is shown below. Since Bagging resamples the original training dataset with replacement, some instance(or data) may be present multiple times while others are left out.</p><br/>
            <pre><strong><em>Classifier generation:</em></strong>

                Let N be the size of the training set.
                for each of t iterations:
                    sample N instances with replacement from the original training set.
                    apply the learning algorithm to the sample.
                    store the resulting classifier.
                
                <strong><em>Classification:</em></strong>
                for each of the t classifiers:
                    predict class of instance using classifier.
                return class that was predicted most often.
                </pre>
            <p>Bagging is one of the Ensemble construction techniques, which is also known as Bootstrap Aggregation. Bootstrap establishes the foundation of the Bagging technique. Bootstrap is a sampling technique in which we select "n" observations from a population of "n" observations. But the selection is entirely random, i.e., each observation can be chosen from the original population so that each observation is equally likely to be selected in each iteration of the bootstrapping process. After the bootstrapped samples are formed, separate models are trained with the bootstrapped samples. In real experiments, the bootstrapped samples are drawn from the training set, and the sub-models are tested using the testing set. The final output prediction is combined across the projections of all the sub-models.</p><br/>
            </div>
      </div>
  </body>
</html>